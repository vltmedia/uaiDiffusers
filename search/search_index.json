{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the documentation for the uaiDiffusers module.</p>"},{"location":"classes/","title":"Overview","text":"<p>These are some classes that help in containing media and other metadata used in the UAIDiffusers module.</p>"},{"location":"classes/#load-values","title":"Load Values","text":""},{"location":"classes/#uaiDiffusers.pipelines.jsonParse","title":"<code>uaiDiffusers.pipelines.jsonParse</code>","text":""},{"location":"classes/#uaiDiffusers.pipelines.jsonParse.DictLoadValue","title":"<code>DictLoadValue(key, dictData, defaultValue)</code>","text":"<p>Load a value from a dict if it exists, otherwise return a default value</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to look for in the dict</p> required <code>dictData</code> <code>dict</code> <p>The dict to look in</p> required <code>defaultValue</code> <code>any</code> <p>The value to return if the key is not found</p> required <p>Returns:</p> Name Type Description <code>any</code> <code>object</code> <p>The value of the key if it exists, otherwise the default value</p>"},{"location":"classes/#uaiDiffusers.pipelines.jsonParse.JsonStringLoadValue","title":"<code>JsonStringLoadValue(key, jsonString, defaultValue)</code>","text":"<p>Load a value from a json string if it exists, otherwise return a default value</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to look for in the dict</p> required <code>jsonString</code> <code>str</code> <p>The json string to load and search</p> required <code>defaultValue</code> <code>any</code> <p>The value to return if the key is not found</p> required <p>Returns:</p> Name Type Description <code>any</code> <code>object</code> <p>The value of the key if it exists, otherwise the default value</p>"},{"location":"classes/#base64-media","title":"Base64 Media","text":""},{"location":"classes/#uaiDiffusers.media.mediaRequestBase","title":"<code>uaiDiffusers.media.mediaRequestBase</code>","text":""},{"location":"classes/#uaiDiffusers.media.mediaRequestBase.MediaRequestBase64","title":"<code>MediaRequestBase64</code>","text":"<p>Class to hold the media as a base64 string along with the prompt used to generate it</p> <code>__init__(media, prompt)</code> <p>Parameters:</p> Name Type Description Default <code>media</code> <code>str</code> <p>The base64 string of the media</p> required <code>prompt</code> <code>str</code> <p>The prompt used to generate the media</p> required"},{"location":"classes/#uaiDiffusers.media.mediaRequestBase.MultipleMediaRequest","title":"<code>MultipleMediaRequest</code>","text":"<p>Class used to hold multiple MediaRequestBase64 objects and return them if needed</p> <code>__init__(media=[])</code> <p>Parameters:</p> Name Type Description Default <code>media</code> <code>list</code> <p>A list of MediaRequestBase64 objects</p> <code>[]</code> <code>addMedia(media)</code> <p>Add a MediaRequestBase64 object to the list of media</p> <p>Parameters:</p> Name Type Description Default <code>media</code> <code>MediaRequestBase64</code> <p>The media to add</p> required <code>toDict()</code> <p>Convert the object to a json string</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The json string</p> <code>toJson()</code> <p>Convert the object to a json string</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The json string</p>"},{"location":"hair/","title":"hair","text":""},{"location":"hair/#uaiDiffusers.hair.HairSegmentation","title":"<code>uaiDiffusers.hair.HairSegmentation</code>","text":""},{"location":"hair/#uaiDiffusers.hair.HairSegmentation.segment_hair","title":"<code>segment_hair(image)</code>","text":"<p>Segments hair from the input image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.array</code> <p>Input image</p> required <p>Returns:</p> Type Description <p>np.array: Hair mask</p>"},{"location":"hair/#uaiDiffusers.hair.download_github_model","title":"<code>uaiDiffusers.hair.download_github_model(model_url, model_path)</code>","text":"<p>Download model from github to a local path</p> <p>Parameters:</p> Name Type Description Default <code>model_url</code> <p>url of the model</p> required <code>model_path</code> <p>local path to save the model</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"pipelines_merger/","title":"merger","text":""},{"location":"pipelines_merger/#uaiDiffusers.pipelines.merger","title":"<code>uaiDiffusers.pipelines.merger</code>","text":""},{"location":"pipelines_merger/#uaiDiffusers.pipelines.merger.CheckpointMergerPipeline","title":"<code>CheckpointMergerPipeline</code>","text":"<p>Merge Multiple checkpoints of the same model into a single checkpoint.</p>"},{"location":"pipelines_merger/#uaiDiffusers.pipelines.merger.MergePytorchBins","title":"<code>MergePytorchBins(model1_path, model2_path, outputPath)</code>","text":"<p>Merge multiple pytorch model .bin files into a single model .bin file. Used mainly to merge Stable Diffusion models.</p> <p>Parameters:</p> Name Type Description Default <code>model1_path</code> <code>str</code> <p>Path to the first model .bin file</p> required <code>model2_path</code> <code>str</code> <p>Path to the second model .bin file</p> required <code>outputPath</code> <code>str</code> <p>Path to save the merged model .bin file</p> required"},{"location":"pipelines_merger/#uaiDiffusers.pipelines.merger.CheckpointMergerPipeline","title":"<code>uaiDiffusers.pipelines.merger.CheckpointMergerPipeline</code>","text":"<p>Merge Multiple checkpoints of the same model into a single checkpoint.</p>"},{"location":"pipelines_merger/#uaiDiffusers.pipelines.merger.CheckpointMergerPipeline._compare_model_configs","title":"<code>uaiDiffusers.pipelines.merger.CheckpointMergerPipeline._compare_model_configs(dict0, dict1)</code>","text":"<p>Compare two model configs and return True if they are the same.</p> <p>Parameters:</p> Name Type Description Default <code>dict0</code> <code>Dict</code> <p>First model config</p> required <code>dict1</code> <code>Dict</code> <p>Second model config</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the two configs are the same, False otherwise.</p>"},{"location":"sadtalk/","title":"sadTalk","text":""},{"location":"sadtalk/#uaiDiffusers.sadTalk.sadTalk","title":"<code>uaiDiffusers.sadTalk.sadTalk</code>","text":""},{"location":"sadtalk/#uaiDiffusers.sadTalk.sadTalk.sadTalkArgs","title":"<code>sadTalkArgs</code>","text":"<code>fromJson(jsonData)</code> <p>Load data from a json object</p> Example <p>args = sadTalkArgs() args.fromJson(jsonData)</p> <p>Parameters:</p> Name Type Description Default <code>jsonData</code> <code>dict</code> <p>A dictionary containing the data to load</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"uaiDiffusers/","title":"uaiDiffusers","text":""},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers","title":"<code>uaiDiffusers.uaiDiffusers</code>","text":""},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.Base64StringToPILImage","title":"<code>Base64StringToPILImage(base64String)</code>","text":"<p>Takes a base64 string and returns a PIL image</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.CV2ToPIL","title":"<code>CV2ToPIL(cv2Image, colorSpace=cv2.COLOR_BGR2RGB)</code>","text":"<p>Convert a CV2 image to a PIL image.</p> <p>Parameters:</p> Name Type Description Default <code>cv2Image</code> <code>cv2.Image</code> <p>The cv2 image to convert.</p> required <code>colorSpace</code> <code>int</code> <p>The  cv2 color space to use.</p> <code>cv2.COLOR_BGR2RGB</code> <p>Returns:</p> Name Type Description <code>image</code> <code>PIL.Image</code> <p>The converted PIL image.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ClampDepth","title":"<code>ClampDepth(rawDepth, thresholdAdd=0.2, maxDepth=1)</code>","text":"<p>Clamp a depth image</p> <p>Parameters:</p> Name Type Description Default <code>rawDepth</code> <code>np.array</code> <p>A depth image</p> required <code>thresholdAdd</code> <code>float</code> <p>The threshold to add.</p> <code>0.2</code> <code>maxDepth</code> <code>int</code> <p>The max depth.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>pilImage</code> <code>PIL.Image</code> <p>The clamped depth image</p> <code>normalizedNumpy</code> <code>np.array</code> <p>The normalized numpy array</p> <code>alpha</code> <code>np.array</code> <p>The alpha numpy array</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ClampImageByDepth","title":"<code>ClampImageByDepth(imagePath, threshold=0.2, maxDepth=1, maskFeather=0.2)</code>","text":"<p>Clamp an image by generate depth</p> <p>Parameters:</p> Name Type Description Default <code>imagePath</code> <code>str | PIL.Image</code> <p>The path OR PIL Image to the image</p> required <code>threshold</code> <code>float</code> <p>The threshold to add</p> <code>0.2</code> <code>maxDepth</code> <code>int</code> <p>The max depth</p> <code>1</code> <code>maskFeather</code> <code>float</code> <p>The mask feather</p> <code>0.2</code> <p>Returns:</p> Name Type Description <code>pilImage</code> <code>PIL.Image</code> <p>The clamped image</p> <code>clampDepthImage</code> <code>PIL.Image</code> <p>The clamped depth image</p> <code>normalizedZeroOne</code> <code>np.array</code> <p>The normalized numpy array</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ConvertNumpyArrayToImage","title":"<code>ConvertNumpyArrayToImage(numpyArray, processType='uint8')</code>","text":"<p>Convert a numpy array to an image</p> <p>Parameters:</p> Name Type Description Default <code>numpyArray</code> <code>np.array</code> <p>A numpy array</p> required <code>processType</code> <code>str</code> <p>The type to convert to. Defaults to \"uint8\".</p> <code>'uint8'</code> <p>Returns:</p> Type Description <p>np.array: The converted numpy array</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ExtractFace","title":"<code>ExtractFace(pilImage)</code>","text":"<p>Extract a face from an image</p> <p>Parameters:</p> Name Type Description Default <code>pilImage</code> <code>PIL.Image</code> <p>The image to extract a face from.</p> required <p>Returns:</p> Name Type Description <code>face</code> <code>PIL.Image</code> <p>The extracted face.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GenerateBackground","title":"<code>GenerateBackground(foregroundImage, mask_, sdRepo='stabilityai/stable-diffusion-2-inpainting', prompt_='A park', negPrompt_='Missing body', imagesToGenerate=1, seed=42, device='cuda')</code>","text":"<p>Generate a background for an image using Stable Diffusion.</p> <p>Parameters:</p> Name Type Description Default <code>foregroundImage</code> <code>PIL.Image</code> <p>The foreground image to use.</p> required <code>mask_</code> <code>PIL.Image</code> <p>The mask to use.</p> required <code>sdRepo</code> <code>str</code> <p>The Stable Diffusion model to use.</p> <code>'stabilityai/stable-diffusion-2-inpainting'</code> <code>prompt_</code> <code>str</code> <p>The prompt to use.</p> <code>'A park'</code> <code>negPrompt_</code> <code>str</code> <p>The negative prompt to use.</p> <code>'Missing body'</code> <code>imagesToGenerate</code> <code>int</code> <p>The number of images to generate.</p> <code>1</code> <code>seed</code> <code>int</code> <p>The seed to use.</p> <code>42</code> <code>device</code> <code>str</code> <p>The device to use.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>image</code> <code>PIL.Image</code> <p>The generated image.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GenerateDepthImage","title":"<code>GenerateDepthImage(pilImage, modelName='Intel/dpt-large')</code>","text":"<p>Generate a depth image from a mono image</p> <p>Parameters:</p> Name Type Description Default <code>pilImage</code> <code>PIL.Image</code> <p>A PIL image</p> required <code>modelName</code> <code>str</code> <p>The model name to use from Huggingface for the DPT model Depth Estimation.</p> <code>'Intel/dpt-large'</code> <p>Returns:</p> Name Type Description <code>depth</code> <code>PIL.Image</code> <p>The depth image</p> <code>output</code> <code>np.array</code> <p>The depth image as a numpy array</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GenerateFace","title":"<code>GenerateFace(inputFaceImage, inputFaceMask, sdRepo='runwayml/stable-diffusion-v1-5', cannyRepo='lllyasviel/sd-controlnet-canny', loraPath='', textualInversion='', customSDBin='', imagesToGenerate=1, steps=20, device='cuda', prompt_='A person', negPrompt_='bad face', low_threshold=100, high_threshold=200, seed=42, pipe_=None)</code>","text":"<p>Generates a face from a face image and a face mask using Stable Diffusion.</p> <p>Parameters:</p> Name Type Description Default <code>inputFaceImage</code> <code>str</code> <p>Path to the face image.</p> required <code>inputFaceMask</code> <code>str</code> <p>Path to the face mask.</p> required <code>sdRepo</code> <code>str</code> <p>Path to the Stable Diffusion model.</p> <code>'runwayml/stable-diffusion-v1-5'</code> <code>cannyRepo</code> <code>str</code> <p>Path to the Canny model.</p> <code>'lllyasviel/sd-controlnet-canny'</code> <code>loraPath</code> <code>str</code> <p>Path to the LoRA model.</p> <code>''</code> <code>textualInversion</code> <code>str</code> <p>Path to the textual inversion model.</p> <code>''</code> <code>customSDBin</code> <code>str</code> <p>Path to the custom Stable Diffusion model.</p> <code>''</code> <code>imagesToGenerate</code> <code>int</code> <p>Number of images to generate.</p> <code>1</code> <code>steps</code> <code>int</code> <p>Number of steps.</p> <code>20</code> <code>device</code> <code>str</code> <p>Device to use.</p> <code>'cuda'</code> <code>prompt_</code> <code>str</code> <p>Text Prompt to use.</p> <code>'A person'</code> <code>negPrompt_</code> <code>str</code> <p>Text Negative prompt to remove certain things from an image.</p> <code>'bad face'</code> <code>low_threshold</code> <code>int</code> <p>Low threshold for Canny.</p> <code>100</code> <code>high_threshold</code> <code>int</code> <p>High threshold for Canny.</p> <code>200</code> <code>seed</code> <code>int</code> <p>Seed to use.</p> <code>42</code> <code>pipe_</code> <code>StableDiffusionPipeline</code> <p>Stable Diffusion pipeline to use.</p> <code>None</code>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GenerateImage","title":"<code>GenerateImage(sdRepo='runwayml/stable-diffusion-v1-5', loraPath='', textualInversion='', customSDBin='', imagesToGenerate=1, steps=20, device='cuda', prompt_='A person', negPrompt_='bad face', seed=42, width=512, height=512, pipe_=None)</code>","text":"<p>Generate an image from a text prompt using Stable Diffusion.</p> <p>Parameters:</p> Name Type Description Default <code>sdRepo</code> <code>str</code> <p>The Stable Diffusion model to use.</p> <code>'runwayml/stable-diffusion-v1-5'</code> <code>loraPath</code> <code>str</code> <p>The path to the LoRA model to use.</p> <code>''</code> <code>textualInversion</code> <code>str</code> <p>The path to the textual inversion model to use. </p> <code>''</code> <code>customSDBin</code> <code>str</code> <p>The path to the custom Stable Diffusion model to use.</p> <code>''</code> <code>imagesToGenerate</code> <code>int</code> <p>The number of images to generate.</p> <code>1</code> <code>steps</code> <code>int</code> <p>The number of steps to use.</p> <code>20</code> <code>device</code> <code>str</code> <p>The device to use.</p> <code>'cuda'</code> <code>prompt_</code> <code>str</code> <p>The prompt to use.</p> <code>'A person'</code> <code>negPrompt_</code> <code>str</code> <p>The negative prompt to use.</p> <code>'bad face'</code> <code>seed</code> <code>int</code> <p>The seed to use.</p> <code>42</code> <code>width</code> <code>int</code> <p>The width of the image to generate.</p> <code>512</code> <code>height</code> <code>int</code> <p>The height of the image to generate.</p> <code>512</code> <code>pipe_</code> <code>StableDiffusionPipeline</code> <p>The Stable Diffusion pipeline to use. Set this to keep from reinitializing models.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>images</code> <code>list</code> <p>A list of images generated.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GetCannyEdges","title":"<code>GetCannyEdges(img, mask=None, low_threshold=50, high_threshold=150)</code>","text":"<p>Get the canny edges of an image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>PIL.Image</code> <p>The image to get the canny edges from.</p> required <code>mask</code> <code>PIL.Image</code> <p>The mask to use.</p> <code>None</code> <code>low_threshold</code> <code>int</code> <p>The low threshold to use.</p> <code>50</code> <code>high_threshold</code> <code>int</code> <p>The high threshold to use.</p> <code>150</code> <p>Returns:</p> Name Type Description <code>canny_image</code> <code>PIL.Image</code> <p>The canny edges of the image.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GetGroupSelfieBodyMask","title":"<code>GetGroupSelfieBodyMask(img, threshold=0.5, depthThreshold=0.8, depthMax=1, maskFeather=1, model_selection=0, maxSize=0)</code>","text":"<p>Get a mask of a group of people in a selfie.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>PIL.Image</code> <p>The image to get the mask from.</p> required <code>threshold</code> <code>float</code> <p>The threshold to use.</p> <code>0.5</code> <code>depthThreshold</code> <code>float</code> <p>The depth threshold to use.</p> <code>0.8</code> <code>depthMax</code> <code>float</code> <p>The depth max to use.</p> <code>1</code> <code>maskFeather</code> <code>int</code> <p>The mask feather to use.</p> <code>1</code> <code>model_selection</code> <code>int</code> <p>The model to use. 0 - 1. 1 is more accurate.</p> <code>0</code> <code>maxSize</code> <code>int</code> <p>The max size to use.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>outImage</code> <code>PIL.Image</code> <p>The masked image of the people.</p> <code>baseimg</code> <code>PIL.Image</code> <p>The input image.</p> <code>imageMask</code> <code>np.array</code> <p>The raw mask.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.GetSelfieBodyMask","title":"<code>GetSelfieBodyMask(img, threshold=0.5, model_selection=0)</code>","text":"<p>Get a mask of a person in a selfie.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>PIL.Image</code> <p>The image to get the mask from.</p> required <code>threshold</code> <code>float</code> <p>The threshold to use.</p> <code>0.5</code> <code>model_selection</code> <code>int</code> <p>The model to use.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>image_seg_mask</code> <code>PIL.Image</code> <p>The mask of the person.</p> <code>composite</code> <code>PIL.Image</code> <p>The masked image.</p> <code>image_seg_maskRaw</code> <code>np.array</code> <p>The raw mask.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ImageToBytes","title":"<code>ImageToBytes(pilImage)</code>","text":"<p>Takes a PIL image and convert it to a BytesIO object</p> <p>Parameters:</p> Name Type Description Default <code>pilImage</code> <code>PIL.Image</code> <p>PIL image</p> required <p>Returns:</p> Name Type Description <code>img_io</code> <code>BytesIO</code> <p>BytesIO object</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ImagesToBase64","title":"<code>ImagesToBase64(pilImage)</code>","text":"<p>Takes a PIL image and convert it to a Base64 string</p> <p>Parameters:</p> Name Type Description Default <code>pilImage</code> <code>PIL.Image</code> <p>PIL image</p> required <p>Returns:</p> Name Type Description <code>base64String</code> <code>str</code> <p>Base64 string</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ImagesToFlaskResponse","title":"<code>ImagesToFlaskResponse(pilImage)</code>","text":"<p>Takes a list of PIL images and returns a zip binary ready to be saved out or sent</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ImagesToZip","title":"<code>ImagesToZip(pilImages)</code>","text":"<p>Takes a list of PIL images and returns a zip binary in memory ready to be saved out or sent</p> <p>Parameters:</p> Name Type Description Default <code>pilImages</code> <code>list</code> <p>list of PIL images</p> required <p>Returns:</p> Name Type Description <code>memory_file</code> <code>BytesIO</code> <p>zip binary in memory</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ImagesToZipFlaskResponse","title":"<code>ImagesToZipFlaskResponse(pilImages)</code>","text":"<p>Takes a list of PIL images and creates a flask response with a zip binary in memory ready to be saved out or sent</p> <p>Parameters:</p> Name Type Description Default <code>pilImages</code> <code>list</code> <p>list of PIL images</p> required <p>Returns:</p> Type Description <p>flask.Response: flask response with a zip binary in memory</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.InvertMask","title":"<code>InvertMask(mask_)</code>","text":"<p>Invert a mask from 0 - 1 to 1 - 0</p> <p>Parameters:</p> Name Type Description Default <code>mask_</code> <code>PIL.Image</code> <p>The mask to invert.</p> required <p>Returns:</p> Name Type Description <code>newmask_</code> <code>PIL.Image</code> <p>The inverted mask.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.MaskOutGeneratedFace","title":"<code>MaskOutGeneratedFace(generatedImage, mask_image)</code>","text":"<p>Mask out a generated face using a 0 - 1 mask</p> <p>Parameters:</p> Name Type Description Default <code>generatedImage</code> <code>PIL.Image</code> <p>The generated image to mask.</p> required <code>mask_image</code> <code>PIL.Image</code> <p>The mask to use.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>PIL.Image</code> <p>The masked image.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.MultiImagesToFlaskResponse","title":"<code>MultiImagesToFlaskResponse(pilImages, prompts)</code>","text":"<p>Takes a list of PIL images and prompts of the same count and returns a MultipleMediaRequest JSON object containing the images and prompts as MediaRequestBase64 objects</p> <p>Parameters:</p> Name Type Description Default <code>pilImages</code> <code>list</code> <p>A list of PIL images</p> required <code>prompts</code> <code>list</code> <p>A list of prompts</p> required <p>Returns:</p> Name Type Description <code>JSONString</code> <code>str</code> <p>A MultipleMediaRequest JSON string</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.NormalizeNumpyArray","title":"<code>NormalizeNumpyArray(numpyArray)</code>","text":"<p>Normalize a numpy array to 0-1</p> <p>Parameters:</p> Name Type Description Default <code>numpyArray</code> <code>np.array</code> <p>A numpy array</p> required <p>Returns:</p> Type Description <p>np.array: The normalized numpy array</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.PILToCV2","title":"<code>PILToCV2(pilImage, colorSpace=cv2.COLOR_RGB2BGR)</code>","text":"<p>Convert a PIL image to a CV2 image.</p> <p>Parameters:</p> Name Type Description Default <code>pilImage</code> <code>PIL.Image</code> <p>The PIL image to convert.</p> required <code>colorSpace</code> <code>int</code> <p>The  cv2 color space to use.</p> <code>cv2.COLOR_RGB2BGR</code> <p>Returns:</p> Name Type Description <code>cv2Image</code> <code>cv2.Image</code> <p>The converted image.</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.ResizeImage","title":"<code>ResizeImage(pilImage, maxWidth=700)</code>","text":"<p>Resize an image based on max width</p> <p>Parameters:</p> Name Type Description Default <code>pilImage</code> <code>PIL.Image</code> <p>A PIL image</p> required <code>maxWidth</code> <code>int</code> <p>The max width</p> <code>700</code> Returs <p>pilImage (PIL.Image): The resized PIL image</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.saveUAIPromptImageJSONObjectToFiles","title":"<code>saveUAIPromptImageJSONObjectToFiles(promptImage, filepath)</code>","text":"<p>Save the image to file</p> <p>Parameters:</p> Name Type Description Default <code>promptImage</code> <code>dict</code> <p>A prompt image from the UAI API</p> required <code>filepath</code> <code>str</code> <p>The path to save the image to</p> required <p>Returns:</p> Type Description <p>PIL.Image: A PIL image</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.uaiPromptImageJSONObjectToPILImage","title":"<code>uaiPromptImageJSONObjectToPILImage(promptImage)</code>","text":"<p>Takes a prompt image from the UAI API and returns a PIL image</p> <p>Parameters:</p> Name Type Description Default <code>promptImage</code> <code>dict</code> <p>A prompt image from the UAI API</p> required <p>Returns:</p> Type Description <p>PIL.Image: A PIL image</p>"},{"location":"uaiDiffusers/#uaiDiffusers.uaiDiffusers.uaiPromptMultiImageJSONObjectToPILImage","title":"<code>uaiPromptMultiImageJSONObjectToPILImage(promptImages)</code>","text":"<p>Takes a prompt image from the UAI API and returns a PIL image</p>"}]}